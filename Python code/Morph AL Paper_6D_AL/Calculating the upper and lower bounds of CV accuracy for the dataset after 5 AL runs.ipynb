{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The upper bound of CV accuracy of any ML models on the tested dataset (initial sampling + 5 AL runs) was calculated as one minus intrinsic error of the dataset. The intrinsic error was estimated as one minus the average CV accuracy of multiple deep neural networks that overfit the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import random\n",
    "from numpy import vstack, hstack\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from Data.datasets import save_obj, load_obj, data_preprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets (2 initial sampling and 5 active learning)\n",
    "df_std = pd.read_csv('Data/005.morph phase mapping.csv')\n",
    "df_std.index = list(df_std['index'])\n",
    "df_score = df_std.filter(['score'], axis = 1)\n",
    "df_std = df_std.drop(['index', 'score'], axis = 1)\n",
    "\n",
    "iterations = [48, 72, 96, 120, 144, 168]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of number of hidden units\n",
    "hidden_unit = np.logspace(start = 0.5, stop = 2, num = 20)\n",
    "hidden_unit = list(set([int(x) for x in hidden_unit]))\n",
    "hidden_unit.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use leave one out to calculate average error, and monitor it with increaseing number of hidden units.\n",
    "error_matrix = []\n",
    "error_std_matrix = []\n",
    "import statistics\n",
    "\n",
    "for iteration in tqdm(iterations):\n",
    "    \n",
    "    error_list = []\n",
    "    error_std_list = []\n",
    "    \n",
    "    df_tested = df_std[:iteration]\n",
    "    df_score_tested = df_score[:iteration]\n",
    "    \n",
    "    for num_unit in hidden_unit:\n",
    "        MLP = MLPClassifier(hidden_layer_sizes = (num_unit,), solver='lbfgs', activation = 'relu')\n",
    "        error = []\n",
    "        for test in df_tested.index:\n",
    "            MLP.fit(np.array(df_tested.drop([test], axis = 'index')), \\\n",
    "                    np.array(df_score_tested.drop([test], axis = 'index')).reshape(len(df_score_tested.drop([test], axis = 'index')),))\n",
    "            error.append(1 - MLP.score(np.array(df_tested.filter([test], axis = 'index')), \\\n",
    "                                       np.array(df_score_tested.filter([test], axis = 'index')).reshape(1,)))\n",
    "            \n",
    "        error_list.append(sum(error)/len(error))\n",
    "        error_std_list.append(statistics.stdev(error))\n",
    "        \n",
    "    error_matrix.append(error_list)\n",
    "    error_std_matrix.append(error_std_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_matrix = np.array(error_matrix)\n",
    "error_std_matrix = np.array(error_std_matrix)\n",
    "\n",
    "df_error = pd.DataFrame(columns = hidden_unit, data = error_matrix)\n",
    "df_error_std = pd.DataFrame(columns = hidden_unit, data = error_std_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "save_obj(df_error, 'intrinsic error rate_5AL_ANN_mean')\n",
    "save_obj(df_error_std, 'intrinsic error rate_5AL_ANN_std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "color_list = ['black', 'purple', 'blue', 'green', 'red', 'orange']\n",
    "\n",
    "for index in df_error.index:\n",
    "    fig = plt.figure(figsize = (6,6))\n",
    "    ax = fig.add_subplot()\n",
    "    \n",
    "    ax.plot(np.log10(hidden_unit), list(df_error.loc[index]*100), c = color_list[index], linewidth = 1, linestyle = 'dashed')\n",
    "    ax.scatter(np.log10(hidden_unit), list(df_error.loc[index]*100), s = 100, c = color_list[index])\n",
    "    \n",
    "#     ax.plot(np.log10(hidden_unit), [17]*19, c = 'gray', linewidth = 1, linestyle = 'dashed')\n",
    "    \n",
    "    ax.set_title('iteration'+ str(index))\n",
    "    ax.set_xlim(0.4,2.1)\n",
    "    ax.set_ylim(10,40)\n",
    "    \n",
    "    ax.set_xticks(np.arange(0.5,2.5,0.5))\n",
    "    ax.set_yticks(np.arange(10,45,5))\n",
    "    ax.tick_params(axis=\"x\", labelsize=15)\n",
    "    ax.tick_params(axis=\"y\", labelsize=15)\n",
    "    # plt.xlabel('log(Hidden units)')\n",
    "    # plt.ylabel('Error')\n",
    "    plt.savefig('Graphs_3/intrinsic error_iteration'+ str(index) +'_2ndversion.svg', format = 'svg', transparent = 'True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the lower bound of prediciton accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = (list(df_score.score).count(1))/168\n",
    "p2 = (list(df_score.score).count(3))/168\n",
    "p3 = (list(df_score.score).count(4))/168\n",
    "gini_imp = p1*(1-p1) + p2*(1-p2) + p3*(1-p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1- gini_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
