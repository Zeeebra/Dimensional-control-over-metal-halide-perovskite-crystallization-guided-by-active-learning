{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import random\n",
    "from numpy import vstack, hstack\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs, make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import entropy_sampling, entropy_sampling, margin_sampling\n",
    "from modAL.batch import uncertainty_batch_sampling\n",
    "from modAL.uncertainty import classifier_uncertainty, classifier_margin, classifier_entropy\n",
    "from modAL.utils.selection import multi_argmax\n",
    "from Models import models, plot, sampling, expgen\n",
    "from Data.datasets import save_obj, load_obj, data_preprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import experiment data, state space, and ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import experiment data and statesapce\n",
    "df_init = load_obj('8 reagent concentration_initial sampling')  # initial concentration\n",
    "df_1stAL = load_obj('8 reagent concentration_1stAL_RF').drop(['uncertainty'], axis = 1) # 1st AL concentration\n",
    "df_2ndAL = load_obj('8 reagent concentration_2ndAL_RF').drop(['uncertainty'], axis = 1) # 2nd AL concentration\n",
    "df_3rdAL = load_obj('8 reagent concentration_3rdAL_RF').drop(['uncertainty'], axis = 1) # 3rd AL concentration\n",
    "df_4thAL = load_obj('8 reagent concentration_4thAL_RF').drop(['uncertainty'], axis = 1) # 4th AL concentration\n",
    "df = pd.concat([df_init, df_1stAL, df_2ndAL, df_3rdAL, df_4thAL])\n",
    "\n",
    "df_init_std = load_obj('8 reagent concentration_initial sampling_standardized') # standardized initial concentration\n",
    "df_1stAL_std = load_obj('8 reagent concentration_1stAL_standardized_RF').drop(['uncertainty'], axis = 1) # standardized 1st AL concentration\n",
    "df_2ndAL_std = load_obj('8 reagent concentration_2ndAL_standardized_RF').drop(['uncertainty'], axis = 1) # standardized 2nd AL concentration\n",
    "df_3rdAL_std = load_obj('8 reagent concentration_3rdAL_standardized_RF').drop(['uncertainty'], axis = 1) # standardized 3rd AL concentration\n",
    "df_4thAL_std = load_obj('8 reagent concentration_4thAL_standardized_RF').drop(['uncertainty'], axis = 1) # standardized 4th AL concentration\n",
    "df_std = pd.concat([df_init_std, df_1stAL_std, df_2ndAL_std, df_3rdAL_std, df_4thAL_std])\n",
    "\n",
    "# Rename the feature names of standardized tested points\n",
    "df_score_init = pd.read_csv('Data/initial sampling_score.csv')\n",
    "df_score_1stAL = pd.read_csv('Data/1st AL_score.csv')\n",
    "df_score_2ndAL = pd.read_csv('Data/2nd AL_score.csv')\n",
    "df_score_3rdAL = pd.read_csv('Data/3rd AL_score.csv')\n",
    "df_score_4thAL = pd.read_csv('Data/4th AL_score.csv')\n",
    "df_score = pd.concat([df_score_init, df_score_1stAL, df_score_2ndAL, df_score_3rdAL, df_score_4thAL])\n",
    "df_score.index = list(df_score['Index']) # rebuilt scores and using \"index\" column as the index of this dataframe\n",
    "df_score = df_score.drop(['Index'], axis = 1) # then remove the \"index\" column\n",
    "\n",
    "print('The index of feature table matches the index of standardized feature table?', (df.index == df_std.index).all())\n",
    "print('The index of score table matches the index of feature table?', (df.index == df_score.index).all())\n",
    "print('Total number of training sets', len(df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load concentration statespace: real one and standardized one\n",
    "df_pool = load_obj('8R homogeneous concentration statespace (Pb2, morph, H2O and FAH constrained)')\n",
    "df_pool_std = load_obj('8R homogeneous concentration statespace_standardized (Pb2, morph, H2O and FAH constrained)')\n",
    "\n",
    "df_pool = df_pool.drop(index = list(df_std.index)) # remove tested points from concentration statespace\n",
    "df_pool_std = df_pool_std.drop(index = list(df_std.index)) # remove tested points from standardized concentration statespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optimized machine learning model (with preset HPs)\n",
    "from Models.models import PearsonVII_kernel\n",
    "cf = load_obj('RandomForestClassifier_best') # choose the ML model\n",
    "# fit the machine learning model\n",
    "cf.fit(df_std,df_score)\n",
    "save_obj(cf,'RFClassifier_4AL_trained_01052021')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active learning: uncertainty query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and visualize prediction uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate prediction uncertainty for all points in the pool.\n",
    "uncernlst = classifier_uncertainty(cf,df_pool_std).reshape((len(df_pool_std),1))\n",
    "\n",
    "# Add uncertainty value to both pool and standardized pool\n",
    "df_pool_std['uncertainty'] = uncernlst\n",
    "df_pool['uncertainty'] = uncernlst\n",
    "\n",
    "# calculate average prediction confidence\n",
    "confidence = classifier_margin(cf,df_pool_std.drop(['uncertainty'], axis = 1))\n",
    "confidence = sum(confidence)/len(confidence)\n",
    "print('Current confidence is', confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot grid point ID vs uncertainty\n",
    "%matplotlib notebook\n",
    "fig = plt.figure(dpi=100)\n",
    "ax = fig.add_subplot()\n",
    "ax.scatter(np.arange(len(df_pool)), df_pool.sort_values(['uncertainty'])['uncertainty'], s = 1, c = 'red')\n",
    "ax.set_xlabel('Grid points ID')\n",
    "ax.set_ylabel('Uncertainty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncern_tier = plot.uncert_bar(uncernlst = uncernlst, ylim = 140000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query pool based on top prediction uncertainty (>50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of query points\n",
    "k = 24 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use diverse mini-batch active learning\n",
    "beta = (uncern_tier[5]+uncern_tier[6]+uncern_tier[7])//k # beta factor selection\n",
    "minbatch = df_pool_std.nlargest(n = beta*k, columns = 'uncertainty') # pick the top k*beta points based on uncertainty\n",
    "\n",
    "# use k-means clustering to find k centorid points out of k*beta points\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters = k, random_state=42)\n",
    "kmeans.fit(minbatch.iloc[:,:6],sample_weight=minbatch.iloc[:,6])\n",
    "centers = kmeans.cluster_centers_ # k centorid points (not necessary to be within k*beta points)\n",
    "\n",
    "# Find the nearest neighbor in the pool to the centorid points of k-means clustering\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "neigh = NearestNeighbors(n_neighbors=1, algorithm='ball_tree') # set neighbor number to be 1\n",
    "neigh.fit(np.array(minbatch.iloc[:,:6])) # fit the model with top k*beta points\n",
    "query_idx = neigh.kneighbors(centers)[1] # find the index of nearest neighbor in the pool\n",
    "\n",
    "# index the pool for query, has to be the sorted version pool_uncerlst, not the initial version\n",
    "df_pool_query = df_pool.loc[minbatch.iloc[query_idx.ravel()].index]\n",
    "df_pool_std_query = df_pool_std.loc[minbatch.iloc[query_idx.ravel()].index]\n",
    "\n",
    "df_pool_query_vol = load_obj('8R homogeneous volume statespace (Pb2, morph, H2O and FAH constrained)')\n",
    "df_pool_query_vol = df_pool_query_vol.loc[minbatch.iloc[query_idx.ravel()].index]\n",
    "\n",
    "save_obj(df_pool_query,\"8 reagent concentration_5thAL_RF\")\n",
    "save_obj(df_pool_std_query,\"8 reagent concentration_5thAL_standardized_RF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot query points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize sampling using PCA\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "pca.fit(df_pool_std.drop(['uncertainty'],axis=1))\n",
    "df_pool_std_PCA_2 = pca.transform(df_pool_std.drop(['uncertainty'],axis=1))\n",
    "df_pool_std_query_3AL_PCA_2 = pca.transform(df_pool_std_query.drop(['uncertainty'],axis=1))\n",
    "\n",
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot whole dataset \n",
    "ax.scatter(df_pool_std_PCA_2[:, 0], \\\n",
    "            df_pool_std_PCA_2[:, 1], \\\n",
    "            c = 'gray', \\\n",
    "            s = 1, alpha = 0.5)\n",
    "\n",
    "# plot the initial sampling\n",
    "ax.scatter(df_pool_std_query_3AL_PCA_2[:, 0], \\\n",
    "            df_pool_std_query_3AL_PCA_2[:, 1], \\\n",
    "            c = 'red', \\\n",
    "            s = 10, alpha = 1)\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.expgen import robot_file_gen_R8\n",
    "robot_file_gen_R8(data = df_pool_query_vol, filename = '8R_5thAL_robotinput')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pool_query"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
