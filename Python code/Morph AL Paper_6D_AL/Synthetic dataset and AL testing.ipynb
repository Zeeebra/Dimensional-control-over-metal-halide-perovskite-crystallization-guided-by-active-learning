{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use active learning algorithm (exact the same) from AL morph phase mapping\n",
    "### on synthetic dataset\n",
    "### The synthetic dataset should be as similar to real morph phase mapping data as possible.\n",
    "### Initial idea is to have: 6 dimensions, 3 classes, unknown cluster number, tunable overlaps and noise level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Models import sampling\n",
    "from Models import AL\n",
    "from Data.datasets import save_obj, load_obj\n",
    "from tqdm import tqdm\n",
    "from modAL.uncertainty import classifier_uncertainty\n",
    "from modAL.uncertainty import classifier_margin\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split, cross_val_score, KFold\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('always')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import experiment datasets\n",
    "df_exp = pd.read_csv('Data/006.morph phase mapping.csv')\n",
    "df_exp.index = list(df_exp['index'])\n",
    "df_exp = df_exp.drop(['index'], axis = 1)\n",
    "\n",
    "class_counts = df_exp['score'].value_counts()\n",
    "# We will use the class weights from experiment data.\n",
    "weights = [class_counts[1]/len(df_exp),class_counts[3]/len(df_exp),class_counts[4]/len(df_exp)]\n",
    "\n",
    "# set the list of dataset parameters.\n",
    "class_sep_lst = [0.2, 0.5, 0.8]\n",
    "flip_y_lst = [0.1, 0.2, 0.5]\n",
    "xv,yv = np.meshgrid(class_sep_lst, flip_y_lst)\n",
    "noise_lst = list(zip(xv.ravel(), yv.ravel()))\n",
    "\n",
    "### Create synthetic dataset with tunable overlps and noise level.\n",
    "dataset_dict = {}\n",
    "from sklearn.datasets import make_classification\n",
    "for noise in noise_lst:\n",
    "    X, y = make_classification(n_samples=50000, n_features=6, n_redundant=0, n_repeated=0,\\\n",
    "                               n_informative=6, n_classes=3, n_clusters_per_class=1,\\\n",
    "                               weights=weights, class_sep = noise[0], random_state=1, flip_y = noise[1])\n",
    "    dataset = pd.DataFrame(columns = ['a','b','c','d','e','f'], data = X)\n",
    "    dataset['score'] = y\n",
    "    dataset_dict[noise] = dataset\n",
    "\n",
    "save_obj(dataset_dict, 'Artificial data_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate AL metrics and performance ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through all the datasets in the dictionary\n",
    "\n",
    "dataset_labeled_dict = {}\n",
    "metrics_dict = {}\n",
    "intrin_err_dict = {}\n",
    "labeled_intrin_err_dict = {}\n",
    "classifier = 'GPC_matern' # SVM_best, RandomForestClassifier_best, GPC_best, GPC_matern\n",
    "\n",
    "cf = load_obj(classifier)\n",
    "cv = KFold(shuffle = True, n_splits=5, random_state=42)\n",
    "#cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "for noise in tqdm(dataset_dict.keys()):\n",
    "    df = dataset_dict[noise]\n",
    "    df_X = df.drop(['score'], axis = 1)\n",
    "    df_y = df.filter(['score'], axis = 1)\n",
    "    \n",
    "    ## select initial training data using Kennard-Stone algorithm\n",
    "    k = 20\n",
    "    init_idx, _ = sampling.kennardstonealgorithm(df_X, k)\n",
    "    print('finished initial sampling')\n",
    "\n",
    "    # use the sample_idx to select initial sampling from df_X\n",
    "    df_X_init = df_X.iloc[init_idx]\n",
    "    df_y_init = df_y.iloc[init_idx]\n",
    "    \n",
    "    ############### Active learning ##############################\n",
    "    iteration_num = 18\n",
    "    number_periter = 5\n",
    "    iteration_step = list(np.arange(k,k+(1+iteration_num)*number_periter,number_periter))\n",
    "    \n",
    "    df_X_labeled = df_X_init.copy()\n",
    "    df_y_labeled = df_y_init.copy()\n",
    "    \n",
    "    for iteration in tqdm(range(iteration_num)):\n",
    "        df_X_unlabeled = df_X.drop(df_X_labeled.index, axis = 'index')\n",
    "        df_X_AL = AL.minibatch_AL(pool = df_X_unlabeled, X_label = df_X_labeled, y_label = df_y_labeled, \\\n",
    "                                  model = cf, numb_periter = number_periter)\n",
    "        df_X_AL = df_X_AL.drop(['uncertainty'], axis = 1)\n",
    "        df_X_labeled = pd.concat([df_X_labeled, df_X_AL])\n",
    "        df_y_labeled = df_y.filter(df_X_labeled.index, axis = 'index')\n",
    "    \n",
    "    df_labeled = pd.concat([df_X_labeled, df_y_labeled], axis = 1)\n",
    "    dataset_labeled_dict[noise] = df_labeled # save the AL labeled dataframe\n",
    "    print('finished active learning')\n",
    "    ################################################################\n",
    "                           \n",
    "    ############### Active learning metrics measurement ##############################\n",
    "    # Calculate overall uncertainty (average uncertainty)\n",
    "    avg_uncert = []\n",
    "    for i in range(len(iteration_step)):\n",
    "        cf.fit(np.array(df_X_labeled[:iteration_step[i]]), np.array(df_y_labeled[:iteration_step[i]]).ravel())\n",
    "        uncert = classifier_uncertainty(cf,np.array(df_X))\n",
    "        avg_uncert.append(sum(uncert)/len(uncert))\n",
    "    print('finished overall uncertainty')\n",
    "    \n",
    "    # Calculate selected accuracy\n",
    "    select_accuracy = []\n",
    "    for i in range(len(iteration_step)-1):\n",
    "        cf.fit(np.array(df_X_labeled[:iteration_step[i]]), np.array(df_y_labeled[:iteration_step[i]]).ravel())\n",
    "        select_accuracy.append(cf.score(np.array(df_X_labeled[iteration_step[i]:iteration_step[i+1]]), \\\n",
    "                                        np.array(df_y_labeled[iteration_step[i]:iteration_step[i+1]]).ravel()))\n",
    "    print('finished selected accuracy')\n",
    "    \n",
    "    # Calculate prediction confidence (average)\n",
    "    pred_confid = []\n",
    "    for i in range(len(iteration_step)):\n",
    "        cf.fit(np.array(df_X_labeled[:iteration_step[i]]), np.array(df_y_labeled[:iteration_step[i]]).ravel())\n",
    "        confidence = classifier_margin(cf, np.array(df_X))\n",
    "        avg_confid = sum(confidence)/len(confidence)\n",
    "        pred_confid.append(avg_confid)\n",
    "    print('finished prediction confidence')\n",
    "                           \n",
    "    # Calculate contradictory information\n",
    "    contra_info = []\n",
    "    incor_confid_lst = []\n",
    "    for i in range(len(iteration_step)-1):\n",
    "        cf.fit(np.array(df_X_labeled[:iteration_step[i]]), np.array(df_y_labeled[:iteration_step[i]]).ravel())\n",
    "        confidence = classifier_margin(cf, np.array(df_X))\n",
    "        avg_confid = sum(confidence)/len(confidence)\n",
    "                           \n",
    "        incor_boolen = cf.predict(np.array(df_X_labeled[iteration_step[i]:iteration_step[i+1]])) != np.array(df_y_labeled[iteration_step[i]:iteration_step[i+1]]).ravel()\n",
    "        if incor_boolen.any():\n",
    "            incor_confid = classifier_margin(cf, df_X_labeled[iteration_step[i]:iteration_step[i+1]][incor_boolen])\n",
    "            \n",
    "            incor_confid_lst.append(incor_confid)\n",
    "            contra_info.append(sum(incor_confid)/avg_confid)\n",
    "        else:\n",
    "            contra_info.append(0) \n",
    "    print('finished contradictory information')\n",
    "    \n",
    "    \n",
    "    # Calculate cross validataion score\n",
    "    df_acc = pd.DataFrame(index = np.arange(len(iteration_step)), columns = ['cross_mean', 'cross_std'])\n",
    "\n",
    "    for i in range(len(iteration_step)):\n",
    "        # for initial sampling, we have one class 1, so when we do cross_val, \n",
    "        # we will face the situation that no class 1 in the training set, that is not gonna work for SVM or GPC\n",
    "        if ((classifier == 'SVM_best') or (classifier == 'GPC_best') or (classifier == 'GPC_matern')) & (i == 0): \n",
    "            df_acc['cross_mean'].loc[i] = 0\n",
    "            df_acc['cross_std'].loc[i] = 0\n",
    "        else:\n",
    "            score = cross_val_score(cf, np.array(df_X_labeled[:iteration_step[i]]), \\\n",
    "                                    np.array(df_y_labeled[:iteration_step[i]]).ravel(), cv=cv)\n",
    "\n",
    "            df_acc['cross_mean'].loc[i] = np.mean(score)\n",
    "            df_acc['cross_std'].loc[i] = np.std(score)\n",
    "    print('finished cross validataion score')\n",
    "\n",
    "    \n",
    "    # Calculate prediction accuracy of the whole dataset\n",
    "    acc_pool = []\n",
    "    for i in range(len(iteration_step)):   \n",
    "        cf.fit(np.array(df_X_labeled[:iteration_step[i]]), np.array(df_y_labeled[:iteration_step[i]]).ravel())\n",
    "        acc_pool.append(cf.score(np.array(df_X), np.array(df_y).ravel()))\n",
    "    print('finished prediction accuracy of the whole dataset')\n",
    "    \n",
    "    # summerize all AL metrics                       \n",
    "    AL_metrics = pd.DataFrame(columns = ['Overal Uncertainty','Prediction Cofindence',\\\n",
    "                                     'Selected Accuracy', 'Contradictory Information',\\\n",
    "                                     'Prediction Accuracy'])\n",
    "    AL_metrics['Overal Uncertainty'] = avg_uncert\n",
    "    AL_metrics['Prediction Cofindence'] = pred_confid\n",
    "    AL_metrics['Selected Accuracy'].iloc[:-1] = select_accuracy\n",
    "    AL_metrics['Contradictory Information'].iloc[1:] = contra_info\n",
    "    AL_metrics['Prediction Accuracy'] = acc_pool\n",
    "    AL_metrics = pd.concat([AL_metrics, df_acc], axis = 1)\n",
    "\n",
    "    metrics_dict[noise] = AL_metrics # save the AL metrics dataframe\n",
    "    ###################################################################################\n",
    "        \n",
    "            \n",
    "    ############### Calculate the intrinsic error rate of whole datasets###############\n",
    "    # Generate a list of number of hidden units\n",
    "    hidden_unit = np.logspace(start = 0.5, stop = 2, num = 20)\n",
    "    hidden_unit = list(set([int(x) for x in hidden_unit]))\n",
    "    hidden_unit.sort()\n",
    "    error_mean = []\n",
    "    error_std = []\n",
    "\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    #cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "    for num_unit in hidden_unit:\n",
    "            MLP = MLPClassifier(hidden_layer_sizes = (num_unit,), solver='lbfgs', activation = 'relu')\n",
    "            accuracy = cross_val_score(MLP, np.array(df_X), np.array(df_y).ravel(), cv=cv)\n",
    "            error = 1-accuracy\n",
    "            error_mean.append(np.mean(error))\n",
    "            error_std.append(np.std(error))\n",
    "\n",
    "    df_intrin_error = pd.DataFrame()\n",
    "    df_intrin_error['hidden_unit'] = hidden_unit\n",
    "    df_intrin_error['error_mean'] = error_mean\n",
    "    df_intrin_error['error_std'] = error_std\n",
    "    \n",
    "    intrin_err_dict[noise] = df_intrin_error\n",
    "    print('finished the intrinsic error rate')\n",
    "    ###################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    ############### Calculate the intrinsic error rate of AL datasets #################   \n",
    "    error_mean = []\n",
    "    error_std = []\n",
    "    \n",
    "    for num_unit in hidden_unit:\n",
    "            MLP = MLPClassifier(hidden_layer_sizes = (num_unit,), solver='lbfgs', activation = 'relu')\n",
    "            accuracy = cross_val_score(MLP, X = np.array(df_X_labeled), \\\n",
    "                                       y = np.array(df_y_labeled).ravel(), \\\n",
    "                                       scoring='accuracy', cv=cv)\n",
    "            error = 1-accuracy\n",
    "            error_mean.append(np.mean(error))\n",
    "            error_std.append(np.std(error))\n",
    "\n",
    "    df_labeled_intrin_error = pd.DataFrame()\n",
    "    df_labeled_intrin_error['hidden_unit'] = hidden_unit\n",
    "    df_labeled_intrin_error['error_mean'] = error_mean\n",
    "    df_labeled_intrin_error['error_std'] = error_std\n",
    "    \n",
    "    labeled_intrin_err_dict[noise] = df_labeled_intrin_error\n",
    "    print('finished the intrinsic error rate of AL datasets')\n",
    "    ###################################################################################\n",
    "\n",
    "############################ save the calculations ###########################################\n",
    "save_obj(dataset_labeled_dict, 'Artificial data_AL labeled dataset_circle_2D_GPC_matern')\n",
    "save_obj(metrics_dict, 'Artificial data_AL metrics_circle_2D_GPC_matern')\n",
    "save_obj(intrin_err_dict, 'Artificial data_intrinsic error_circle_2D_GPC_matern')   \n",
    "save_obj(labeled_intrin_err_dict, 'Artificial data_labeled_intrinsic error_circle_2D_GPC_matern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict[0].to_csv('metrics_circle_2D_0_GPC_matern.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate F1, recall, and precision in each iteraction of active learning ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through all the datasets in the dictionary\n",
    "\n",
    "dataset_labeled_dict = {}\n",
    "model_metrics_lst = {}\n",
    "model_metrics_pool_lst = {}\n",
    "\n",
    "classifier = 'GPC_matern' # SVM_best, RandomForestClassifier_best, GPC_best, GPC_matern\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "cf = load_obj(classifier)\n",
    "cv = KFold(shuffle = True, n_splits=5, random_state=42)\n",
    "#cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "for noise in tqdm(dataset_dict.keys()):\n",
    "    df = dataset_dict[noise]\n",
    "    df_X = df.drop(['score'], axis = 1)\n",
    "    df_y = df.filter(['score'], axis = 1)\n",
    "    \n",
    "    ## select initial training data using Kennard-Stone algorithm\n",
    "    k = 20\n",
    "    init_idx, _ = sampling.kennardstonealgorithm(df_X, k)\n",
    "    print('finished initial sampling')\n",
    "\n",
    "    # use the sample_idx to select initial sampling from df_X\n",
    "    df_X_init = df_X.iloc[init_idx]\n",
    "    df_y_init = df_y.iloc[init_idx]\n",
    "    \n",
    "    ############### Active learning ##############################\n",
    "    iteration_num = 18\n",
    "    number_periter = 5\n",
    "    iteration_step = list(np.arange(k,k+(1+iteration_num)*number_periter,number_periter))\n",
    "    \n",
    "    df_X_labeled = df_X_init.copy()\n",
    "    df_y_labeled = df_y_init.copy()\n",
    "    \n",
    "    for iteration in range(iteration_num):\n",
    "        df_X_unlabeled = df_X.drop(df_X_labeled.index, axis = 'index')\n",
    "        df_X_AL = AL.minibatch_AL(pool = df_X_unlabeled, X_label = df_X_labeled, y_label = df_y_labeled, \\\n",
    "                                  model = cf, numb_periter = number_periter)\n",
    "        df_X_AL = df_X_AL.drop(['uncertainty'], axis = 1)\n",
    "        df_X_labeled = pd.concat([df_X_labeled, df_X_AL])\n",
    "        df_y_labeled = df_y.filter(df_X_labeled.index, axis = 'index')\n",
    "    \n",
    "    df_labeled = pd.concat([df_X_labeled, df_y_labeled], axis = 1)\n",
    "    dataset_labeled_dict[noise] = df_labeled # save the AL labeled dataframe\n",
    "    print('finished active learning')\n",
    "    ################################################################\n",
    "                           \n",
    "    ############### Active learning metrics measurement ##############################  \n",
    "    # Calculate cross validataion score (f1, recall, precision)\n",
    "    df_metric = pd.DataFrame(index = np.arange(len(iteration_step)), \\\n",
    "                          columns = ['f1_mean', 'f1_std', 'precision_mean', 'precision_std', 'recall_mean', 'recall_std'])\n",
    "\n",
    "    for i in range(len(iteration_step)):\n",
    "        # for initial sampling, we have one class 1, so when we do cross_val, \n",
    "        # we will face the situation that no class 1 in the training set, that is not gonna work for SVM or GPC\n",
    "        if ((classifier == 'SVM_best') or (classifier == 'GPC_best') or (classifier == 'GPC_matern')) & (i == 0): \n",
    "            f1 = 0\n",
    "            precision = 0\n",
    "            recall = 0\n",
    "        else:\n",
    "            f1 = cross_val_score(cf, np.array(df_X_labeled[:iteration_step[i]]), \\\n",
    "                                 np.array(df_y_labeled[:iteration_step[i]]).ravel(), cv=cv, \\\n",
    "                                 scoring = 'f1')\n",
    "            \n",
    "            precision = cross_val_score(cf, np.array(df_X_labeled[:iteration_step[i]]), \\\n",
    "                                        np.array(df_y_labeled[:iteration_step[i]]).ravel(), cv=cv, \\\n",
    "                                       scoring = 'precision')\n",
    "            \n",
    "            recall = cross_val_score(cf, np.array(df_X_labeled[:iteration_step[i]]), \\\n",
    "                                    np.array(df_y_labeled[:iteration_step[i]]).ravel(), cv=cv, \\\n",
    "                                    scoring = 'recall')\n",
    "\n",
    "            df_metric['f1_mean'].loc[i] = np.mean(f1)\n",
    "            df_metric['f1_std'].loc[i] = np.std(f1)\n",
    "            df_metric['precision_mean'].loc[i] = np.mean(precision)\n",
    "            df_metric['precision_std'].loc[i] = np.std(precision)\n",
    "            df_metric['recall_mean'].loc[i] = np.mean(recall)\n",
    "            df_metric['recall_std'].loc[i] = np.std(recall)\n",
    "    \n",
    "    model_metrics_lst[noise] = df_metric\n",
    "    print('finished cross validataion score')\n",
    "\n",
    "    \n",
    "    # Calculate metrics of the whole dataset\n",
    "    f1_pool = []\n",
    "    precision_pool = []\n",
    "    recall_pool = []\n",
    "    \n",
    "    df_metric_pool = pd.DataFrame(index = np.arange(len(iteration_step)), \\\n",
    "                                  columns = ['f1', 'precision', 'recall'])\n",
    "    \n",
    "    for i in range(len(iteration_step)):   \n",
    "        cf.fit(np.array(df_X_labeled[:iteration_step[i]]), np.array(df_y_labeled[:iteration_step[i]]).ravel())\n",
    "        df_y_predict = cf.predict(df_X)\n",
    "        \n",
    "        f1_pool.append(f1_score(np.array(df_y).ravel(), df_y_predict.ravel(), pos_label = 1))\n",
    "        precision_pool.append(precision_score(np.array(df_y).ravel(), df_y_predict.ravel(), pos_label = 1))\n",
    "        recall_pool.append(recall_score(np.array(df_y).ravel(), df_y_predict.ravel(), pos_label = 1))\n",
    "        \n",
    "    df_metric_pool['f1'] = f1_pool\n",
    "    df_metric_pool['precision'] = precision_pool\n",
    "    df_metric_pool['recall'] = recall_pool\n",
    "    \n",
    "    print('finished prediction metrics of the whole dataset')\n",
    "\n",
    "\n",
    "    model_metrics_pool_lst[noise] = df_metric_pool\n",
    "    ###################################################################################\n",
    "        \n",
    "            \n",
    "############################ save the calculations ###########################################\n",
    "save_obj(model_metrics_lst, 'Artificial data_AL labeled_CV metrics_circle_6D_GPC_matern')\n",
    "save_obj(model_metrics_pool_lst, 'Artificial data_pool_true metrics_circle_6D_GPC_matern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tSNE conversion ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = load_obj('Artificial data_dataset')\n",
    "dataset_tSNE_dict = {}\n",
    "\n",
    "for noise in tqdm(dataset_dict.keys()):\n",
    "    df = dataset_dict[noise]\n",
    "    df_X = df.drop(['score'], axis = 1)\n",
    "    df_y = df.filter(['score'], axis = 1)\n",
    "    \n",
    "    ################### tSNE transformation of the df #################################  \n",
    "    # using tSNE to transform the artificial data to 2D\n",
    "    from sklearn.manifold import TSNE\n",
    "    X_tsne = TSNE(n_components=2, perplexity = 50, random_state = 42).fit_transform(np.array(df_X)) \n",
    "    df_X_tSNE = pd.DataFrame(columns = ['dim 1', 'dim 2'], data = X_tsne)\n",
    "    df_tSNE = pd.concat([df_X_tSNE, df_y], axis = 1)\n",
    "    dataset_tSNE_dict[noise] = df_tSNE\n",
    "    ###################################################################################\n",
    "    \n",
    "save_obj(dataset_tSNE_dict, 'Artificial data_tSNE dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
