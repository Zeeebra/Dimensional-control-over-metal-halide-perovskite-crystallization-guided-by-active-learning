{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import random\n",
    "from numpy import vstack, hstack\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs, make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import entropy_sampling, entropy_sampling, margin_sampling\n",
    "from modAL.batch import uncertainty_batch_sampling\n",
    "from modAL.uncertainty import classifier_uncertainty, classifier_margin, classifier_entropy\n",
    "from modAL.utils.selection import multi_argmax\n",
    "from Models import models, plot, sampling, expgen\n",
    "from Data.datasets import save_obj, load_obj, data_preprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import experiment data, state space, and ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import experiment data and statesapce\n",
    "df_init = load_obj('8 reagent concentration_initial sampling')  # tested points for plotting\n",
    "df_1stAL = load_obj('8 reagent concentration_1stAL_RF').drop(['uncertainty'], axis = 1) # tested points for plotting\n",
    "df = pd.concat([df_init, df_1stAL])\n",
    "\n",
    "df_init_std = load_obj('8 reagent concentration_initial sampling_standardized') # standardized tested points for training ML model\n",
    "df_1stAL_std = load_obj('8 reagent concentration_1stAL_standardized_RF').drop(['uncertainty'], axis = 1) # standardized tested points for training ML model\n",
    "df_std = pd.concat([df_init_std, df_1stAL_std])\n",
    "\n",
    "# Rename the feature names of standardized tested points\n",
    "df_score_init = pd.read_csv('Data/initial sampling_score.csv')\n",
    "df_score_1stAL = pd.read_csv('Data/1st AL_score.csv')\n",
    "df_score = pd.concat([df_score_init, df_score_1stAL])\n",
    "df_score.index = list(df_score['Index']) # rebuilt scores and using \"index\" column as the index of this dataframe\n",
    "df_score = df_score.drop(['Index'], axis = 1) # then remove the \"index\" column\n",
    "\n",
    "# load concentration statespace: real one and standardized one\n",
    "df_pool = load_obj('8R homogeneous concentration statespace (Pb2, morph, H2O and FAH constrained)')\n",
    "df_pool_std = load_obj('8R homogeneous concentration statespace_standardized (Pb2, morph, H2O and FAH constrained)')\n",
    "\n",
    "print('The index of score table matches the index of feature table?', (df.index == df_score.index).all())\n",
    "df_pool = df_pool.drop(index = list(df_std.index)) # remove tested points from concentration statespace\n",
    "df_pool_std = df_pool_std.drop(index = list(df_std.index)) # remove tested points from standardized concentration statespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optimized machine learning model (with preset HPs)\n",
    "from Models.models import PearsonVII_kernel\n",
    "cf = load_obj('RandomForestClassifier_best') # choose the ML model\n",
    "# fit the machine learning model\n",
    "cf.fit(df_std,df_score)\n",
    "save_obj(cf,'RFClassifier_2AL_trained_12152020')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active learning: uncertainty query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate prediction uncertainty for all points in the pool.\n",
    "uncernlst = classifier_uncertainty(cf,df_pool_std).reshape((len(df_pool_std),1))\n",
    "\n",
    "# Add uncertainty value to both pool and standardized pool\n",
    "df_pool_std['uncertainty'] = uncernlst\n",
    "df_pool['uncertainty'] = uncernlst\n",
    "# save_obj(pool,'pool coordinations_after test exp_10152020 + uncertainty')\n",
    "\n",
    "confidence = classifier_margin(cf,df_pool_std.drop(['uncertainty'], axis = 1))\n",
    "confidence = sum(confidence)/len(confidence)\n",
    "print('Current confidence is', confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot grid point ID vs uncertainty\n",
    "%matplotlib notebook\n",
    "fig = plt.figure(dpi=100)\n",
    "ax = fig.add_subplot()\n",
    "ax.scatter(np.arange(len(df_pool)), df_pool.sort_values(['uncertainty'])['uncertainty'], s = 1, c = 'red')\n",
    "ax.set_xlabel('Grid points ID')\n",
    "ax.set_ylabel('Uncertainty')\n",
    "# plt.savefig('Graphs/Grid point ID (1 to 1674) vs uncertainty_1stAL.svg', format = \"svg\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot gird point in 3D space, colored by uncertainty\n",
    "%matplotlib notebook\n",
    "cm = plt.cm.get_cmap('jet')\n",
    "fig = plt.figure(figsize=(7, 4.5),dpi=100)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "im = ax.scatter(df_pool.sort_values(['uncertainty'])['Pb'],\\\n",
    "                df_pool.sort_values(['uncertainty'])['morph'],\\\n",
    "                df_pool.sort_values(['uncertainty'])['FAH'],\\\n",
    "                c = df_pool.sort_values(['uncertainty'])['uncertainty'],\\\n",
    "                cmap = cm, alpha = 0.5)\n",
    "cax = fig.add_axes()\n",
    "fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "#plt.savefig('Graphs/Grid point 3D with uncertainty_1stAL.svg', format = \"svg\", transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot uncertainty distribution.\n",
    "tier0 = []\n",
    "tier1 = []\n",
    "tier2 = []\n",
    "tier3 = []\n",
    "tier4 = []\n",
    "tier5 = []\n",
    "tier6 = []\n",
    "tier7 = []\n",
    "for i in uncernlst.ravel():\n",
    "    if i>=0.7:\n",
    "        tier7.append(i)\n",
    "    elif i>=0.6:\n",
    "        tier6.append(i)\n",
    "    elif i>=0.5:\n",
    "        tier5.append(i)\n",
    "    elif i>=0.4:\n",
    "        tier4.append(i)\n",
    "    elif i>=0.3:\n",
    "        tier3.append(i)\n",
    "    elif i>=0.2:\n",
    "        tier2.append(i)\n",
    "    elif i>=0.1:\n",
    "        tier1.append(i)\n",
    "    else:\n",
    "        tier0.append(i)\n",
    "\n",
    "groupnumber = [len(tier0), len(tier1),len(tier2),len(tier3),len(tier4),len(tier5),len(tier6),len(tier7)]\n",
    "fig, ax = plt.subplots()\n",
    "plt.bar(list(range(8)),groupnumber)\n",
    "plt.ylim(0, 140000)\n",
    "plt.xlabel('uncertainty')\n",
    "plt.ylabel('counts')\n",
    "plt.xticks(list(range(8)), ('<0.1', '0.1-0.2', '0.2-0.3', '0.3-0.4', '0.4-0.5', \"0.5-0.6\", \"0.6-0.7\", \">=0.7\"))\n",
    "#plt.savefig('Graphs/Grid point uncertainty distribution.svg', format = \"svg\", transparent=True)\n",
    "plt.show()\n",
    "\n",
    "print(\"number of points with >0.5 uncertainty (_2ndAL): \", len(tier5)+len(tier6)+len(tier7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query certain number of points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use diverse mini-batch active learning\n",
    "k = 24 # number of query points\n",
    "beta = (len(tier5)+len(tier6)+len(tier7))//k # beta factor selection\n",
    "\n",
    "minbatch = df_pool_std.nlargest(n = beta*k, columns = 'uncertainty') # pick the top k*beta points based on uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use k-means clustering to find k centorid points out of k*beta points\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters = k, random_state=42)\n",
    "kmeans.fit(minbatch.iloc[:,:6],sample_weight=minbatch.iloc[:,6])\n",
    "centers = kmeans.cluster_centers_ # k centorid points (not necessary to be within k*beta points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the nearest neighbor in the pool to the centorid points of k-means clustering\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "neigh = NearestNeighbors(n_neighbors=1, algorithm='ball_tree') # set neighbor number to be 1\n",
    "neigh.fit(np.array(minbatch.iloc[:,:6])) # fit the model with top k*beta points\n",
    "query_idx = neigh.kneighbors(centers)[1] # find the index of nearest neighbor in the pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_idx.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index the pool for query, has to be the sorted version pool_uncerlst, not the initial version\n",
    "df_pool_query = df_pool.loc[minbatch.iloc[query_idx.ravel()].index]\n",
    "df_pool_std_query = df_pool_std.loc[minbatch.iloc[query_idx.ravel()].index]\n",
    "\n",
    "df_pool_query_vol = load_obj('8R homogeneous volume statespace (Pb2, morph, H2O and FAH constrained)')\n",
    "df_pool_query_vol = df_pool_query_vol.loc[minbatch.iloc[query_idx.ravel()].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(df_pool_query,\"8 reagent concentration_2ndAL_RF\")\n",
    "save_obj(df_pool_std_query,\"8 reagent concentration_2ndAL_standardized_RF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot query points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models import plot\n",
    "\n",
    "df_plot = df_pool_query.filter(['Pb','morph','FAH'])\n",
    "df_plot['crystal score'] = [5]*24\n",
    "\n",
    "%matplotlib notebook\n",
    "plot.plot3d2d(point=np.array(df_plot), x_range = [0, 3], y_range = [0, 5], z_range = [0, 16], \\\n",
    "              xy_loc = -6, xz_loc = 2, yz_loc = -2,\\\n",
    "              x_step = 0.5, y_step = 0.5, z_step = 2, elev = 30, azim = -60, name = '8R_1stAL_pb_morph_FAH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize sampling using PCA\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "pca.fit(df_pool_std.drop(['uncertainty'],axis=1))\n",
    "df_pool_std_PCA_2 = pca.transform(df_pool_std.drop(['uncertainty'],axis=1))\n",
    "df_pool_std_query_2AL_PCA_2 = pca.transform(df_pool_std_query.drop(['uncertainty'],axis=1))\n",
    "\n",
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot whole dataset \n",
    "ax.scatter(df_pool_std_PCA_2[:, 0], \\\n",
    "            df_pool_std_PCA_2[:, 1], \\\n",
    "            c = 'gray', \\\n",
    "            s = 1, alpha = 0.5)\n",
    "\n",
    "# plot the initial sampling\n",
    "ax.scatter(df_pool_std_query_2AL_PCA_2[:, 0], \\\n",
    "            df_pool_std_query_2AL_PCA_2[:, 1], \\\n",
    "            c = 'red', \\\n",
    "            s = 10, alpha = 1)\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.expgen import robot_file_gen_R8\n",
    "robot_file_gen_R8(data = df_pool_query_vol, filename = '8R_2ndAL_robotinput')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pool_query_vol"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
